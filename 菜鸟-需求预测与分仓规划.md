---
title: 菜鸟-需求预测与分仓规划
date: 2017-04-05 23:19:45
tags: [npm, hexo, github]
Categories: 比赛
---

大赛题目链接：https://tianchi.aliyun.com/competition/information.htm?spm=5176.100067.5678.2.aY5MAf&raceId=231530

### 思路整理：

#### 1. 理解题意：

题目的意思就是根据商品从20141010到20151227的全国和区域分仓数据，后面两周（20151228-20160110）的全国和区域分仓目标库存。目标是预测尽量准，如果不能百分百准确，希望是损失最小，即达到库存成本最小化(无论是补多补少)。

#### 2. 原始数据分析

有963个商品，每个商品又不同的销售周期，后期可能需要考虑商品销售高峰时间段(距离出售时间)，标签应该是未来14内的销量，特征可以是原始特征+业务特征+时间影响因子+窗口特征

#### 3. 特征碎碎念：

* 原始特征：

​       日期，商品ID，叶子类目ID，大类目ID，品牌ID，供应商ID

​       浏览次数，流量UV，被加购次数，加购人次，收藏夹人次

​       拍下笔数，拍下金额，拍下件数，拍下UV

​       成交金额，成交笔数，成交件数，成交人次

​       直通车引导浏览次数，淘宝客引导浏览次数，搜索引导浏览次数，聚划算引导浏览次数

​       直通车引导浏览人次，淘宝客引导浏览人次，搜索引导浏览人次，聚划算引导浏览人次

​       非聚划算支付笔数，非聚划算支付金额，非聚划算支付件数，非聚划算支付人次

* 可能潜在的特征：

  + 业务特征

    成交比率：成交人次／流量UV ；成交成功率：成交笔数／拍下笔数

    商品折扣率：成交价格 / 拍下价格；

    单件成交价格：成交金额／成交件数；单笔成交价格：成交金额／成交笔数

    非聚划支付转化率：非聚划算支付人次／流量UV

    非聚划售出占比：非聚划算支付件数／成交件数

    非聚划支付占比：非聚划算支付金额／成交金额

    单笔价值：非聚划算支付金额／非聚划算支付笔数

  + 时间影响因子特征

    最大天数：商品最早出现历史数据的日期距离要预测的那一周的天数。

    最小天数：商品最后一次出现历史数据的日期距离要预测的那一周的天数。

    时间因子：商品时间距离20151228天数。

  + 时间窗口特征（根据记忆曲线，取商品 前 1、2、3、4、7、14、21、28天）

    根据商品、叶子类目、大类目、品牌、供应商进行分组，提取前X天的非聚划算支付人次、浏览数、加购数的平均值、最小值、标准差作为特征

* 缺失值补0

#### 4. 模型探索：

- 数据准备：用mysql处理，17万个样本（滑窗取得），400多维指标（见上文潜在指标分析）

- 数据预处理：去除双11，双12，去除异常值(3西格玛)，去除可能为负的数据。

- 选择两种类型模型分别尝试：

  1. 传统算法Hotwinters----时间序列分析(核心思想是三次指数平滑，包含趋势性季节性，可控性优于arima)

  2. 机器学习agboost, rf, gbdt, adaboost再融合———只做了xgb和gbrt和rf，外加一个规则模型。

     1）xgb优点：1.正则化提升；2.可以实现并行处理；3.允许用户定义**自定义优化目标和评价标准**；4.内置处理缺失值的规则；5.一直分裂到指定的最大深度(max_depth)，然后回过头来剪枝；6.允许在每一轮boosting迭代中使用交叉验证；7.可以在上一轮的结果上继续训练。

     2）gbdt,rf

     3）由于牵扯到时间窗口，没有进行交叉验证，应该考虑添加一个时间因子指标，可供交叉验证。

- 规则处理：预测不可能百分百准确，应该做到损失尽量小

  如果补少成本大于补多成本，则应该预测多一点，取单模型预测结果中的最大值再乘以1.1，反之取单模型预测结果中的最小值再乘以0.9。最终以 0.85model + 0.15rule融合。

  对于上线不满四周的item，直接采用最近两周的平均销量*14预测获得。

- 测试集和训练集


​       训练集选择从2015-07-01以后的数据进行每14天全量滑动，总样本有963*180，取最后四周时间进行滑动，作为测试集。

### 代码目录说明

1. raw_data：原始数据

2. Data：过程数据文件

3. raw_data原始数据

4. Data过程数据文件

5. raw_data原始数据

6. Data过程数据文件

7. mysql_process文件：

   数据处理文件，先运行***preprocess***，再运行***dataprocess***

8. model文件包含：

   ***data_preprocessing.py ：***进行简单的数据处理，添加分割字段，标题等。

   ***model.py：***包含有gbrt，rf，xgb三个模型，其中参数有少许微调。

   ***combine.py ：***gbrt，rf，xgb三个模型得到结果的组合。

   ***rule.py：***规则模型，包含布多补少算法，以及加权规则。

   ***ensemble&estimate.py：***机器学习模型与规则模型融合，输出结果，评估结果。

9. result结果文件：

​       ***final_estimate.csv：***为最终结果集

​       ***04_09.csv：***为带有误差及方差的预测集合。


### 节点把控

1. 第一天下午拿到题目，晚上有事就看了一下。4.3
2. 第二天了解了下题意，进行电脑环境的配置，大致翻了几个网页，论坛，博客，GitHub之类的。4.4
3. 第三天开始正式探索题目意思以及数据。4.5
4. 第四天准备数据4.6
5. 第五天训练模型4.7
6. 第六天调优4.8


###  后续提升

- 对label进行log变换
- 增加特征、增加训练集
- 增加时间序列分析模型

### 踩过的坑

1. mysql导出数据，各种出错，详情见博客。

2. python函数merge区分字符与整型，例如"1"和1不同，merge时会出错；函数传参，有时候会不灵。

3. hive单机简直是个坑，动不动就内存超限，估计是我Hadoop一套太菜了，到时候得好好学学。

4. get新技能：

   merge,concat,apply,sort_values,groupby和egg连用。


### 欢迎大神交流指正

### 参考：

https://devhub.io/repos/wepe-CaiNiao-DemandForecast-StoragePlaning

xgb: http://www.dongcoder.com/detail-56275.html

好思路：

http://blog.csdn.net/bryan__/article/details/51685639



http://www.lizhuoqing.com/2017/02/27/%E9%98%BF%E9%87%8C%E5%A4%A9%E6%B1%A0%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%AF%94%E8%B5%9B%E6%80%BB%E7%BB%93%EF%BC%882016%E8%8F%9C%E9%B8%9F%E9%9C%80%E6%B1%82%E9%A2%84%E6%B5%8B%E4%B8%8E%E5%88%86%E4%BB%93/



https://devhub.io/repos/wepe-CaiNiao-DemandForecast-StoragePlaning





### 



